#!/bin/sh
# Downloads all posts from a user on Eka's Portal (g4)


if [ $# -eq 0 ]
then
    echo "Usage: $0 <artist> [optional cookie file]"
    exit 1
fi


# Get amount of pages
echo "Checking amount of pages..."
pages=$(curl -s "https://aryion.com/g4/latest/$1" | grep -Eo "Page 1 of [[:digit:]]*" | head -n1 | awk 'NF>1{print $NF}')


echo "Getting list of IDs from $pages pages... (this might take a while)"
# Get list of ID
list=\
"$(
# Get all pages
curl -s "https://aryion.com/g4/latest/$1&p=[1-$pages]" | \
# Get all submissions
grep view/ | \
# Get only what's inside of each href
sed -n 's/.*href="\([^"]*\).*/\1/p' | \
# Get rid of everything before last slash, leaving only IDs
grep -o '[^/]*$' | \
# Add download URL to start of each ID
awk '{ print "https://aryion.com/g4/data.php?id=" $0; }' | \
# Newlines to spaces
tr '\n' ' '
)"


# Start downloading!
curl --cookie "$2" --remote-name-all --fail --remote-time --remote-header-name $list